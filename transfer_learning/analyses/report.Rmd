---
params:
  project_name: "Snapshot Serengeti"
  project_id: "ss"
  ts_id: "201707271307"
  model_name: "Species Top26"
  model_id: "ss_species_26"
title: "`r paste('',params$project_name,' - ',params$model_name)`"
author: "Marco Willi"
date: "8 August 2017"
output: pdf_document
---

```{r setup, include=FALSE}
# output: html_document
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=10, fig.height=10, fig.align = "center")
# knitr::opts_chunk$set(dev = 'pdf')
# Options
path_main <- "D:/Studium_GD/Zooniverse/Data/transfer_learning_project/"
project_id <- params$project_id
pred_file <- paste(params$model_id,"_",params$ts_id,"_preds_val",sep="")
log_file <- paste(params$model_id,"_",params$ts_id,"_training",sep="")
model <- params$model_id
project_name <- params$project_name
subject_set <- paste("val_subject_set_",params$model_id,sep="")
```


```{r, include=FALSE, echo=FALSE}
############################ -
# Libraries ----
############################ -

library(ggplot2)
library(reshape2)
library(plyr)
library(dplyr)
library(jsonlite)
library(jpeg)
library(grid)
library(gridExtra)


############################ -
# Paths ----
############################ -

path_logs <- paste(path_main,"logs/",project_id,"/",sep="")
path_save <- paste(path_main,"save/",project_id,"/",sep="")
path_db <- paste(path_main,"db/",project_id,"/",sep="")
path_scratch <- paste(path_main,"scratch/",project_id,"/",sep="")
path_figures <- paste(path_main,"save/",project_id,"/figures/",sep="")

############################ -
# Read Data ----
############################ -

preds <- read.csv(paste(path_save,pred_file,".csv",sep=""))
preds$model <- model
log <- read.csv(paste(path_logs,log_file,".log",sep=""))
                 
# remove top5 accuracy if less than 6 different classes
if (nlevels(preds$y_true) < 6){
  log <- log[,!grepl("top_k",colnames(log))]
}
```

## Introduction

This report shows some results obtained from training and evaluating a convolutional neural network (CNN) on images from `r params$project_name`. The data has been divided into a training, a validation and a test set. The model has only seen the training set, while the validation set was used to monitor the model during training time. The test set is completely independent and is only used to report final results.
For each image the model calculates n (the number of classes) pseudo probabilities (values between 0 and 1, softmax transformation). We assign the class with the highest output value to each image.

## Overview

A short overview on the number of classes & their distribution.

### Class distribution Validation Set
```{r, echo=FALSE}
class_dist <- group_by(preds,y_true) %>%
  summarise(n_obs = n()) %>%
  mutate(p_obs=n_obs / sum(n_obs))

gg <- ggplot(class_dist, aes(x=reorder(y_true, n_obs), y=n_obs)) + geom_bar(stat="identity") +
  theme_light() +
  ggtitle(paste("Class Distribution \nmodel: ", model,sep="")) +
  ylab("# of samples") +
  xlab("") +
  coord_flip() +
  geom_text(aes(label=paste(" ",round(p_obs,4)*100," %",sep="")), hjust="left") +
  scale_y_continuous(limits=c(0,max(class_dist$n_obs) * 1.1))
gg
```


## Training Progress

Some metrics which visualize the progress of training the convolutional neural network.

* Training Epoch: The number of full passes over the whole training set.
* Top-1 Accuracy: Percent of all images when the image corresponds to the class the model deems to be the most likely class.
* Top-5 Accuracy: Percent of all images when the image corresponds to the class the model predicts in its top 5 classes (common metric in machine learning)
* Loss: This is the metric, the algorithm tries to minimize during training. Validation loss is used to decide when to stop training the model.


```{r, echo=FALSE}
############### -
# Plot LOG File
############### -

# reformat data
log_rf <- melt(log, id.vars = c("epoch"))
log_rf$group <- ifelse(grepl(pattern = "loss", log_rf$variable),"Loss",
                       ifelse(grepl(pattern = "top", log_rf$variable),"Top-5 Accuracy","Top-1 Accuracy"))
log_rf$variable <- revalue(log_rf$variable, c("acc"="Top-1 Accuracy - Train", "loss"="Train Loss",
                          "val_acc"="Top-1 Accuracy - Validation", "val_loss"="Validation Loss",
                          "sparse_top_k_categorical_accuracy"="Top-5 Accuracy - Train",
                          "val_sparse_top_k_categorical_accuracy"="Top-5 Accuracy - Validation"))
log_rf$set <- ifelse(grepl(pattern = "Train", log_rf$variable),"Train","Validation")

gg <- ggplot(log_rf, aes(x=epoch, y=value, colour=set, group=variable)) + geom_line(lwd=1.5) +
  theme_light() +
  ggtitle(paste("Accuracy/Loss of Train / Validation along training epochs\nmodel: ", model,sep="")) +
  xlab("Training Epoch") +
  ylab("Loss / Accuracy (%)") +
  facet_grid(group~., scales = "free") +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 10)) +
  scale_color_brewer(type = "div", palette = "Set1")
gg
```

## Accuracy Measurements
### Accuracy per Class on Validation set

```{r, echo=FALSE}
# accuracy per true class
preds_class <- group_by(preds,y_true) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
  mutate(accuracy=matches/n)
gg <- ggplot(preds_class, aes(x=reorder(y_true, accuracy),y=accuracy, label=paste("Acc: ", round(accuracy,3)," Obs: ", n))) + 
  geom_bar(stat="identity", colour="gray") +
  theme_light() +
  ggtitle(paste("Validation Accuracy for Classes\nmodel: ", model,"\nall images",sep="")) +
  xlab("") +
  ylab("Accuracy (%)") +
  coord_flip() +
  geom_text(size = 3, position = position_stack(vjust = 0.5), colour="white")
gg
```

### Accuracy per Class - only high confidence predictions

```{r, echo=FALSE}
# take only with most confidence and Threshold
preds_max_p <- group_by(preds, subject_id) %>% summarise(max_p=max(p))
preds_1 <- left_join(preds, preds_max_p, by="subject_id") %>% filter(p==max_p) %>% filter(p>0.95)

# accuracy per true class
preds_class <- group_by(preds_1,y_true) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
  mutate(accuracy=matches/n)

# get total class numbers and join
class_numbers <- group_by(preds, y_true) %>% summarise(n_total=n_distinct(subject_id))

preds_class <- left_join(preds_class, class_numbers, by="y_true") %>% mutate(p_high_threshold=round(n/n_total,2)*100)

gg <- ggplot(preds_class, aes(x=reorder(y_true, accuracy),y=accuracy, 
                              label=paste("Acc: ", round(accuracy,3)," Obs: ", n," / ",n_total," (",p_high_threshold," %)",sep=""))) + 
  geom_bar(stat="identity", colour="gray") +
  theme_light() +
  ggtitle(paste("Validation Accuracy for Classes\nmodel: ", model,"\nsubject level and only > 95% confidence",sep="")) +
  xlab("Species") +
  ylab("Accuracy (%)") +
  coord_flip() +
  geom_text(size = 3, position = position_stack(vjust = 0.5), colour="white")
gg
```


### Accuracy for different model thresholds

#### Overall
```{r, echo=FALSE}
# Overall View

# take only with most confidence and Threshold
preds_max_p <- group_by(preds, subject_id) %>% summarise(max_p=max(p))

res <- NULL

# thresholds to test
thresholds <- seq(min(preds$p),0.99,by=0.025)
for (ii in seq_along(thresholds)){
  
  preds_1 <- left_join(preds, preds_max_p, by="subject_id") %>% filter(p==max_p) %>% filter(p>=thresholds[ii])
  
  # accuracy overall
  preds_class <- group_by(preds_1) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
    mutate(accuracy=matches/n)
  
  # get total class numbers and join
  class_numbers <- group_by(preds) %>% summarise(n_total=n_distinct(subject_id))
  
  preds_class$p_high_threshold <- sapply(round(preds_class$n/class_numbers$n_total,2),function(x){min(1,x)})
  preds_class$threshold <- thresholds[ii]
  
  res[[ii]] <- preds_class
}
res2 <- do.call(rbind,res)
res3 <- melt(data = res2, id.vars = c("threshold")) %>% filter(variable %in% c("accuracy", "p_high_threshold"))


gg <- ggplot(res3, aes(x=threshold, y=value, colour=variable, group=variable)) + geom_line(lwd=2)  + 
  theme_light() +
  ggtitle(paste("Accuracy vs Modle Threshold\nmodel: ", model,sep="")) + 
  xlab("Model Threshold") +
  ylab("Accuracy / Share (%)") +
  scale_x_continuous(limit = c(min(res3$threshold),1)) +
  scale_y_continuous(breaks = seq(min(res3$value),1,0.02)) +
  scale_color_brewer(type = "qual", guide =  guide_legend(title=NULL),  
                     labels = c("Accuracy (%)", "Proportion of Images\nAbove Threshold (%)")) +
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14),
        legend.text = element_text(size=12))
gg
```

#### Per Class
```{r, echo=FALSE, fig.width=14, fig.height=12}
# take only with most confidence and Threshold
preds_max_p <- group_by(preds, subject_id) %>% summarise(max_p=max(p))

res <- NULL

# thresholds to test
thresholds <- seq(min(preds$p),0.95,by=0.05)
for (ii in seq_along(thresholds)){

  preds_1 <- left_join(preds, preds_max_p, by="subject_id") %>% filter(p==max_p) %>% filter(p>=thresholds[ii])
  head(preds_1)
  
  # accuracy per true class
  preds_class <- group_by(preds_1,y_true) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
    mutate(accuracy=matches/n)
  preds_class
  
  # get total class numbers and join
  class_numbers <- group_by(preds, y_true) %>% summarise(n_total=n_distinct(subject_id))
  
  preds_class <- left_join(preds_class, class_numbers, by="y_true") %>% mutate(p_high_threshold=round(n/n_total,2))
  preds_class$threshold <- thresholds[ii]
  preds_class$p_high_threshold <- ifelse(preds_class$p_high_threshold>1,1,preds_class$p_high_threshold)
  
  res[[ii]] <- preds_class
}
res2 <- do.call(rbind,res)
res3 <- melt(data = res2, id.vars = c("threshold", "y_true")) %>% filter(variable %in% c("accuracy", "p_high_threshold"))

gg <- ggplot(res3, aes(x=threshold, y=value, colour=variable, group=variable)) + geom_line(lwd=2)  + 
  theme_light() +
  facet_wrap("y_true") +
  ggtitle(paste("Accuracy vs Modle Threshold\nmodel: ", model,sep="")) + 
  xlab("Model Threshold") +
  ylab("Accuracy / Share (%)") +
  scale_x_continuous(limit = c(min(res3$threshold),1)) +
  scale_y_continuous(breaks = seq(min(res3$value),1,0.1)) +
  scale_color_brewer(type = "qual", guide =  guide_legend(title=NULL),  
                     labels = c("Accuracy (%)", "Proportion of Images\nAbove Threshold (%)")) +
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14),
        legend.text = element_text(size=12))
gg
```

## Confusion Matrix

Predicted labels vs true labels.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# empty confusion matrix
conf_empty <- expand.grid(levels(preds$y_true),levels(preds$y_true))
names(conf_empty) <- c("y_true","y_pred")

# accuracy per true class
class_sum <- group_by(preds,y_true) %>% summarise(n_class = n())
conf <- group_by(preds,y_true, y_pred) %>% summarise(n = n()) %>% left_join(class_sum) %>%
  mutate(p_class=n / n_class)
conf <- left_join(conf_empty, conf,by=c("y_true","y_pred")) %>% mutate(p_class = ifelse(is.na(p_class),0,p_class))

gg <- ggplot(conf, aes(x=y_pred, y=y_true)) + 
  geom_tile(aes(fill = p_class), colour = "black") + theme_bw() +
  ggtitle(paste("Confusion Matrix\nmodel: ", model,sep="")) + 
  scale_fill_gradient2(low="blue", mid="yellow", high="red", midpoint=0.5, guide =  FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_text(aes(label = round(p_class, 2)),cex=2.5) +
  ylab("True") +
  xlab("Predicted") +
  scale_x_discrete()
gg
```

## Distribution of Predicted Values
```{r, echo=FALSE, fig.width=12, fig.height=12}
preds_dist <- preds
preds_dist$correct <- factor(ifelse(preds_dist$y_true!=preds_dist$y_pred,0,1))
gg <- ggplot(preds_dist, aes(x=p, fill=correct)) + geom_density(alpha=0.3) + 
  facet_wrap("y_true", scales="free") +
  ggtitle(paste("Predicted values - density distribution\nmodel: ", model,sep="")) + 
  theme_light() +
  xlab("Predicted Value") +
  ylab("Density") + 
  scale_fill_brewer(type = "qual",direction = -1)
gg
```


## Examples

Below are some randomly selected correct / missclassified predictions. Included is the confidence of the model for the different classes.

```{r, echo=FALSE}
############################ -
# Read Data ----
############################ -
preds <- read.csv(paste(path_save,pred_file,".csv",sep=""))
preds$model <- model
# check levels
missing_levels <- setdiff(levels(preds$y_true), levels(preds$y_pred))
levels(preds$y_pred) <- c(levels(preds$y_pred),missing_levels)
preds$correct <- factor(ifelse(preds$y_true==preds$y_pred,1,0))
subjects <- jsonlite::read_json(paste(path_db,subject_set,".json",sep=""), simplifyVector = FALSE, flatten=TRUE)
preds_original <- preds

```


### Correct Predictions

```{r, echo=FALSE, message=FALSE, warning=FALSE}
############################ -
# Find some missclassifications ----
############################ -

random_wrongs <- filter(preds,correct==1)
set.seed(23)
random_wrongs <- random_wrongs[sample(size = 10,x=dim(random_wrongs)[1]),]

############################ -
# Plot missclassifications ----
############################ -

for (ii in 1:10){
  
  id <- paste(random_wrongs[ii,"subject_id"])
  preds <- random_wrongs[ii,"preds_all"]
  preds <- fromJSON(paste("[",gsub(pattern = "'", "\"", x=as.character(preds[[1]])),"]",sep=""))
  preds <- melt(preds,value.name = "prob",variable.name = "class")
  sub <- subjects[id]
  url_file <- unlist(sub[[id]]['urls'])[1]
  label <- unlist(sub[[id]]['label'])
  file_name <- paste(path_scratch,"image_",ii,".jpeg",sep="")
  download.file(url=url_file, destfile = file_name, mode = 'wb')
  img <- readJPEG(file_name)
  gg1 <- ggplot(data.frame(x=0:1,y= 0:1),aes(x=x,y=y), geom="blank") +
  annotation_custom(rasterGrob(img, width=unit(1,"npc"), height=unit(1,"npc")), 
                    -Inf, Inf, -Inf, Inf) + theme_minimal() +
  theme(axis.title = element_blank(), axis.text = element_blank()) +
  theme(plot.margin = unit(c(0.7,0.9,0,0.9), "cm"))
  
  # keep only top 5
  preds <- preds[order(preds$prob, decreasing = TRUE)[1:min(5,dim(preds)[1])],]
  gg2 <- ggplot(preds, aes(x=reorder(class, prob),y=prob)) + geom_bar(stat="identity", fill="lightblue") +
    coord_flip() +
    theme_light() +
    ylab("Predicted Probability") +
    xlab("") +
    theme(axis.text.y=element_blank(), axis.text.x=element_text(size=16),
          axis.title.x=element_text(size=16),
          axis.title.y=element_text(size=16),
          axis.ticks.y = element_blank()) +
    geom_text(aes(label=class, y=0.05), size=5,fontface="bold", vjust="middle", hjust="left") +
    theme(plot.margin = unit(c(0.7,0.9,0.5,0.9), "cm"), panel.border=element_rect(fill=NA)) +
    scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
    labs(x=NULL)
  title=textGrob(label = label,gp=gpar(fontsize=20,fontface="bold"), vjust=1)
  grid.arrange(gg1,gg2,top=title)
  grid.rect(.5,.5,width=unit(0.99,"npc"), height=unit(0.99,"npc"), 
            gp=gpar(lwd=3, fill=NA, col="black"))

}
```



### Missclassifications
Here are some random missclassifications.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
############################ -
# Find some missclassifications ----
############################ -
preds <- preds_original
random_wrongs <- filter(preds, correct==0)
set.seed(23)
random_wrongs <- random_wrongs[sample(size = 10,x=dim(random_wrongs)[1]),]

############################ -
# Plot missclassifications ----
############################ -

for (ii in 1:10){
  
  id <- paste(random_wrongs[ii,"subject_id"])
  preds <- random_wrongs[ii,"preds_all"]
  preds <- fromJSON(paste("[",gsub(pattern = "'", "\"", x=as.character(preds[[1]])),"]",sep=""))
  preds <- melt(preds,value.name = "prob",variable.name = "class")
  sub <- subjects[id]
  url_file <- unlist(sub[[id]]['urls'])[1]
  label <- unlist(sub[[id]]['label'])
  file_name <- paste(path_scratch,"image_",ii,".jpeg",sep="")
  download.file(url_file, destfile = file_name, mode = 'wb')
  img <- readJPEG(file_name)
  gg1 <- ggplot(data.frame(x=0:1,y= 0:1),aes(x=x,y=y), geom="blank") +
  annotation_custom(rasterGrob(img, width=unit(1,"npc"), height=unit(1,"npc")), 
                    -Inf, Inf, -Inf, Inf) + theme_minimal() +
  theme(axis.title = element_blank(), axis.text = element_blank()) +
  theme(plot.margin = unit(c(0.7,0.9,0,0.9), "cm"))
  
  # keep only top 5
  preds <- preds[order(preds$prob, decreasing = TRUE)[1:min(5,dim(preds)[1])],]
  gg2 <- ggplot(preds, aes(x=reorder(class, prob),y=prob)) + geom_bar(stat="identity", fill="lightblue") +
    coord_flip() +
    theme_light() +
    ylab("Predicted Probability") +
    xlab("") +
    theme(axis.text.y=element_blank(), axis.text.x=element_text(size=16),
          axis.title.x=element_text(size=16),
          axis.title.y=element_text(size=16),
          axis.ticks.y = element_blank()) +
    geom_text(aes(label=class, y=0.05), size=5,fontface="bold", vjust="middle", hjust="left") +
    theme(plot.margin = unit(c(0.7,0.9,0.5,0.9), "cm"), panel.border=element_rect(fill=NA)) +
    scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
    labs(x=NULL)
  title=textGrob(label = label,gp=gpar(fontsize=20,fontface="bold"), vjust=1)
  grid.arrange(gg1,gg2,top=title)
  grid.rect(.5,.5,width=unit(0.99,"npc"), height=unit(0.99,"npc"), 
            gp=gpar(lwd=3, fill=NA, col="black"))

}
```