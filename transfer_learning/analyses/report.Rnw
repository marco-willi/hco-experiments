\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{bm}
\usepackage{pdfpages}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\setlength{\parindent}{2em}

\pagestyle{fancy}
\fancyhf{}
\rhead{Marco Willi}
\lhead{Snapshot Serengeti - Top 26 Species}
\rfoot{Page \thepage}

\title{Snapshot Serengeti - Top 26 Species}
\author{Marco Willi}
\begin{document}
\maketitle

<<echo=FALSE,message=FALSE>>=
# Parameters
path_main <- "D:/Studium_GD/Zooniverse/Data/transfer_learning_project/"
project_id <- "ss"
model_id <- "ss_species_26"
ts_id <-  "201707271307"
project_name <- "Snapshot Serengeti"

# more parameters
subject_set <- paste("val_subject_set_",model_id,sep="")
pred_file <- paste(model_id,"_",ts_id,"_preds_val",sep="")
log_file <- paste(model_id,"_",ts_id,"_training",sep="")
model <- model_id

fig_width_normal = 8
fig_height_normal = 8
fig_width_large = 12
fig_height_large = 12
@


<<echo=FALSE,message=FALSE>>=
############################ -
# Libraries ----
############################ -

library(ggplot2)
library(reshape2)
library(plyr)
library(dplyr)
library(jsonlite)
library(jpeg)
library(grid)
library(gridExtra)

############################ -
# Paths ----
############################ -

path_logs <- paste(path_main,"logs/",project_id,"/",sep="")
path_save <- paste(path_main,"save/",project_id,"/",sep="")
path_db <- paste(path_main,"db/",project_id,"/",sep="")
path_scratch <- paste(path_main,"scratch/",project_id,"/",sep="")
path_figures <- paste(path_main,"save/",project_id,"/figures/",sep="")

############################ -
# Read Data ----
############################ -

preds <- read.csv(paste(path_save,pred_file,".csv",sep=""))
preds$model <- model
log <- read.csv(paste(path_logs,log_file,".log",sep=""))
                 
# remove top5 accuracy if less than 6 different classes
if (nlevels(preds$y_true) < 6){
  log <- log[,!grepl("top_k",colnames(log))]
}
@


\section{Introduction}

This report shows some results obtained from training and evaluating a convolutional neural network (CNN) on images from \Sexpr{project_name}. The data has been divided into a training, a validation and a test set. The model has only seen the training set, while the validation set was used to monitor the model during training time. The test set is completely independent and is only used to report final results.
For each image the model calculates n (the number of classes) pseudo probabilities (values between 0 and 1, softmax transformation). We assign the class with the highest output value to each image.

\section{Overview}

A short overview on the number of classes \& their distribution.

\subsection{Class distribution on Validation Set}
<<echo=FALSE>>=
class_dist <- group_by(preds,y_true) %>%
  summarise(n_obs = n()) %>%
  mutate(p_obs=n_obs / sum(n_obs))

gg <- ggplot(class_dist, aes(x=reorder(y_true, n_obs), y=n_obs)) + geom_bar(stat="identity") +
  theme_light() +
  ggtitle(paste("Class Distribution \nmodel: ", model,sep="")) +
  ylab("# of samples") +
  xlab("") +
  coord_flip() +
  geom_text(aes(label=paste(" ",round(p_obs,4)*100," %",sep="")), hjust="left") +
  scale_y_continuous(limits=c(0,max(class_dist$n_obs) * 1.1))
gg
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Class Distribution}
\label{fig_cl_dist}
\end{center}
\end{figure}


\newpage
\section{Training Progress}

Some metrics which visualize the progress of training the convolutional neural network.

\begin{itemize}
  \item Training Epoch: The number of full passes over the whole training set.
  \item Top-1 Accuracy: Percent of all images when the image corresponds to the class the model deems to be the most likely class.
  \item Top-5 Accuracy: Percent of all images when the image corresponds to the class the model predicts in its top 5 classes (common metric in machine learning)
  \item Loss: This is the metric, the algorithm tries to minimize during training. Validation loss is used to decide when to stop training the model.
\end{itemize}


<<echo=FALSE>>=
############### -
# Plot LOG File
############### -

# reformat data
log_rf <- melt(log, id.vars = c("epoch"))
log_rf$group <- ifelse(grepl(pattern = "loss", log_rf$variable),"Loss",
                       ifelse(grepl(pattern = "top", log_rf$variable),"Top-5 Accuracy","Top-1 Accuracy"))
log_rf$variable <- revalue(log_rf$variable, c("acc"="Top-1 Accuracy - Train", "loss"="Train Loss",
                          "val_acc"="Top-1 Accuracy - Validation", "val_loss"="Validation Loss",
                          "sparse_top_k_categorical_accuracy"="Top-5 Accuracy - Train",
                          "val_sparse_top_k_categorical_accuracy"="Top-5 Accuracy - Validation"))
log_rf$set <- ifelse(grepl(pattern = "Train", log_rf$variable),"Train","Validation")

gg <- ggplot(log_rf, aes(x=epoch, y=value, colour=set, group=variable)) + geom_line(lwd=1.5) +
  theme_light() +
  ggtitle(paste("Accuracy/Loss of Train / Validation along training epochs\nmodel: ", model,sep="")) +
  xlab("Training Epoch") +
  ylab("Loss / Accuracy (%)") +
  facet_grid(group~., scales = "free") +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 10)) +
  scale_color_brewer(type = "div", palette = "Set1")
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Training metrics}
\label{fig_log}
\end{center}
\end{figure}

\newpage
\section{Accuracy Measurements}
\subsection{Accuracy per Class on Validation set}

<<echo=FALSE>>=
# accuracy per true class
preds_class <- group_by(preds,y_true) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
  mutate(accuracy=matches/n)
gg <- ggplot(preds_class, aes(x=reorder(y_true, accuracy),y=accuracy, label=paste("Acc: ", round(accuracy,3)," Obs: ", n))) + 
  geom_bar(stat="identity", colour="gray") +
  theme_light() +
  ggtitle(paste("Validation Accuracy for Classes\nmodel: ", model,"\nall images",sep="")) +
  xlab("") +
  ylab("Accuracy (%)") +
  coord_flip() +
  geom_text(size = 3, position = position_stack(vjust = 0.5), colour="white")
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Accuracy per class}
\label{fig_acc_per_class}
\end{center}
\end{figure}

\newpage
\subsection{Accuracy per Class - only high confidence predictions}

<<echo=FALSE>>=
# take only with most confidence and Threshold
preds_max_p <- group_by(preds, subject_id) %>% summarise(max_p=max(p))
preds_1 <- left_join(preds, preds_max_p, by="subject_id") %>% filter(p==max_p) %>% filter(p>0.95)

# accuracy per true class
preds_class <- group_by(preds_1,y_true) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
  mutate(accuracy=matches/n)

# get total class numbers and join
class_numbers <- group_by(preds, y_true) %>% summarise(n_total=n_distinct(subject_id))

preds_class <- left_join(preds_class, class_numbers, by="y_true") %>% mutate(p_high_threshold=round(n/n_total,2)*100)

gg <- ggplot(preds_class, aes(x=reorder(y_true, accuracy),y=accuracy, 
                              label=paste("Acc: ", round(accuracy,3)," Obs: ", n," / ",n_total," (",p_high_threshold," %)",sep=""))) + 
  geom_bar(stat="identity", colour="gray") +
  theme_light() +
  ggtitle(paste("Validation Accuracy for Classes\nmodel: ", model,"\nsubject level and only > 95% confidence",sep="")) +
  xlab("Species") +
  ylab("Accuracy (%)") +
  coord_flip() +
  geom_text(size = 3, position = position_stack(vjust = 0.5), colour="white")
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Accuracy per class - high confidence predictions only}
\label{fig_acc_per_class_hc}
\end{center}
\end{figure}

\newpage
\subsection{Accuracy for different model thresholds}

\subsubsection{Overall}
<<echo=FALSE>>=
# Overall View

# take only with most confidence and Threshold
preds_max_p <- group_by(preds, subject_id) %>% summarise(max_p=max(p))

res <- NULL

# thresholds to test
thresholds <- seq(min(preds$p),0.99,by=0.025)
for (ii in seq_along(thresholds)){
  
  preds_1 <- left_join(preds, preds_max_p, by="subject_id") %>% filter(p==max_p) %>% filter(p>=thresholds[ii])
  
  # accuracy overall
  preds_class <- group_by(preds_1) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
    mutate(accuracy=matches/n)
  
  # get total class numbers and join
  class_numbers <- group_by(preds) %>% summarise(n_total=n_distinct(subject_id))
  
  preds_class$p_high_threshold <- sapply(round(preds_class$n/class_numbers$n_total,2),function(x){min(1,x)})
  preds_class$threshold <- thresholds[ii]
  
  res[[ii]] <- preds_class
}
res2 <- do.call(rbind,res)
res3 <- melt(data = res2, id.vars = c("threshold")) %>% filter(variable %in% c("accuracy", "p_high_threshold"))


gg <- ggplot(res3, aes(x=threshold, y=value, colour=variable, group=variable)) + geom_line(lwd=2)  + 
  theme_light() +
  ggtitle(paste("Accuracy vs Modle Threshold\nmodel: ", model,sep="")) + 
  xlab("Model Threshold") +
  ylab("Accuracy / Share (%)") +
  scale_x_continuous(limit = c(min(res3$threshold),1)) +
  scale_y_continuous(breaks = seq(min(res3$value),1,0.02)) +
  scale_color_brewer(type = "qual", guide =  guide_legend(title=NULL),  
                     labels = c("Accuracy (%)", "Proportion of Images\nAbove Threshold (%)")) +
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14),
        legend.text = element_text(size=12))
@


\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Accuracy per thresholds}
\label{fig_acc_per_threshold}
\end{center}
\end{figure}

\newpage
\subsubsection{Per Class}
<<echo=FALSE>>=
# take only with most confidence and Threshold
preds_max_p <- group_by(preds, subject_id) %>% summarise(max_p=max(p))

res <- NULL

# thresholds to test
thresholds <- seq(min(preds$p),0.95,by=0.05)
for (ii in seq_along(thresholds)){

  preds_1 <- left_join(preds, preds_max_p, by="subject_id") %>% filter(p==max_p) %>% filter(p>=thresholds[ii])
  head(preds_1)
  
  # accuracy per true class
  preds_class <- group_by(preds_1,y_true) %>% summarise(matches = sum(y_true == y_pred), n = n()) %>%
    mutate(accuracy=matches/n)
  preds_class
  
  # get total class numbers and join
  class_numbers <- group_by(preds, y_true) %>% summarise(n_total=n_distinct(subject_id))
  
  preds_class <- left_join(preds_class, class_numbers, by="y_true") %>% mutate(p_high_threshold=round(n/n_total,2))
  preds_class$threshold <- thresholds[ii]
  preds_class$p_high_threshold <- ifelse(preds_class$p_high_threshold>1,1,preds_class$p_high_threshold)
  
  res[[ii]] <- preds_class
}
res2 <- do.call(rbind,res)
res3 <- melt(data = res2, id.vars = c("threshold", "y_true")) %>% filter(variable %in% c("accuracy", "p_high_threshold"))

gg <- ggplot(res3, aes(x=threshold, y=value, colour=variable, group=variable)) + geom_line(lwd=2)  + 
  theme_light() +
  facet_wrap("y_true") +
  ggtitle(paste("Accuracy vs Modle Threshold\nmodel: ", model,sep="")) + 
  xlab("Model Threshold") +
  ylab("Accuracy / Share (%)") +
  scale_x_continuous(limit = c(min(res3$threshold),1)) +
  scale_y_continuous(breaks = seq(min(res3$value),1,0.1)) +
  scale_color_brewer(type = "qual", guide =  guide_legend(title=NULL),  
                     labels = c("Accuracy (%)", "Proportion of Images\nAbove Threshold (%)")) +
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14),
        legend.text = element_text(size=12))
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_large,fig.height=fig_height_large>>=
gg
@
\caption{Accuracy per thresholds and class}
\label{fig_acc_per_threshold_class}
\end{center}
\end{figure}

\newpage
\section{Confusion Matrix}

Predicted labels vs true labels.
<<echo=FALSE, message=FALSE, warning=FALSE>>=
# empty confusion matrix
conf_empty <- expand.grid(levels(preds$y_true),levels(preds$y_true))
names(conf_empty) <- c("y_true","y_pred")

# accuracy per true class
class_sum <- group_by(preds,y_true) %>% summarise(n_class = n())
conf <- group_by(preds,y_true, y_pred) %>% summarise(n = n()) %>% left_join(class_sum) %>%
  mutate(p_class=n / n_class)
conf <- left_join(conf_empty, conf,by=c("y_true","y_pred")) %>% mutate(p_class = ifelse(is.na(p_class),0,p_class))

gg <- ggplot(conf, aes(x=y_pred, y=y_true)) + 
  geom_tile(aes(fill = p_class), colour = "black") + theme_bw() +
  ggtitle(paste("Confusion Matrix\nmodel: ", model,sep="")) + 
  scale_fill_gradient2(low="blue", mid="yellow", high="red", midpoint=0.5, guide =  FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_text(aes(label = round(p_class, 2)),cex=2.5) +
  ylab("True") +
  xlab("Predicted") +
  scale_x_discrete()
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Confusion matrix of predicted (x axis) vs true (y axis) labels}
\label{fig_conf}
\end{center}
\end{figure}

\newpage
\section{Distribution of Predicted Values}
<<echo=FALSE>>=
preds_dist <- preds
preds_dist$correct <- factor(ifelse(preds_dist$y_true!=preds_dist$y_pred,0,1))
gg <- ggplot(preds_dist, aes(x=p, fill=correct)) + geom_density(alpha=0.3) + 
  facet_wrap("y_true", scales="free") +
  ggtitle(paste("Predicted values - density distribution\nmodel: ", model,sep="")) + 
  theme_light() +
  xlab("Predicted Value") +
  ylab("Density") + 
  scale_fill_brewer(type = "qual",direction = -1)
@

\begin{figure}[hbp]
\begin{center}
<<echo=FALSE,fig.width=fig_width_normal,fig.height=fig_height_normal>>=
gg
@
\caption{Distribution of predicted values, separately for correct and incorrect predictions}
\label{fig_dist_output}
\end{center}
\end{figure}

\newpage
\section{Examples}

Below are some randomly selected correct / missclassified predictions. Included is the confidence of the model for the different classes.

<<echo=FALSE>>=
############################ -
# Read Data ----
############################ -
preds <- read.csv(paste(path_save,pred_file,".csv",sep=""))
preds$model <- model
# check levels
missing_levels <- setdiff(levels(preds$y_true), levels(preds$y_pred))
levels(preds$y_pred) <- c(levels(preds$y_pred),missing_levels)
preds$correct <- factor(ifelse(preds$y_true==preds$y_pred,1,0))
subjects <- jsonlite::read_json(paste(path_db,subject_set,".json",sep=""), simplifyVector = FALSE, flatten=TRUE)
preds_original <- preds

@


\subsection{Correct Predictions}

<<echo=FALSE, message=FALSE, warning=FALSE>>=
############################ -
# Find some missclassifications ----
############################ -

random_wrongs <- filter(preds,correct==1)
set.seed(23)
random_wrongs <- random_wrongs[sample(size = 10,x=dim(random_wrongs)[1]),]

############################ -
# Plot missclassifications ----
############################ -

for (ii in 1:10){
  
  id <- paste(random_wrongs[ii,"subject_id"])
  preds <- random_wrongs[ii,"preds_all"]
  preds <- fromJSON(paste("[",gsub(pattern = "'", "\"", x=as.character(preds[[1]])),"]",sep=""))
  preds <- melt(preds,value.name = "prob",variable.name = "class")
  sub <- subjects[id]
  url_file <- unlist(sub[[id]]['urls'])[1]
  label <- unlist(sub[[id]]['label'])
  file_name <- paste(path_scratch,"image_",ii,".jpeg",sep="")
  download.file(url=url_file, destfile = file_name, mode = 'wb')
  img <- readJPEG(file_name)
  gg1 <- ggplot(data.frame(x=0:1,y= 0:1),aes(x=x,y=y), geom="blank") +
  annotation_custom(rasterGrob(img, width=unit(1,"npc"), height=unit(1,"npc")), 
                    -Inf, Inf, -Inf, Inf) + theme_minimal() +
  theme(axis.title = element_blank(), axis.text = element_blank()) +
  theme(plot.margin = unit(c(0.7,0.9,0,0.9), "cm"))
  
  # keep only top 5
  preds <- preds[order(preds$prob, decreasing = TRUE)[1:min(5,dim(preds)[1])],]
  gg2 <- ggplot(preds, aes(x=reorder(class, prob),y=prob)) + geom_bar(stat="identity", fill="lightblue") +
    coord_flip() +
    theme_light() +
    ylab("Predicted Probability") +
    xlab("") +
    theme(axis.text.y=element_blank(), axis.text.x=element_text(size=16),
          axis.title.x=element_text(size=16),
          axis.title.y=element_text(size=16),
          axis.ticks.y = element_blank()) +
    geom_text(aes(label=class, y=0.05), size=5,fontface="bold", vjust="middle", hjust="left") +
    theme(plot.margin = unit(c(0.7,0.9,0.5,0.9), "cm"), panel.border=element_rect(fill=NA)) +
    scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
    labs(x=NULL)
  title=textGrob(label = label,gp=gpar(fontsize=20,fontface="bold"), vjust=1)
  grid.arrange(gg1,gg2,top=title)
  grid.rect(.5,.5,width=unit(0.99,"npc"), height=unit(0.99,"npc"), 
            gp=gpar(lwd=3, fill=NA, col="black"))

}
@





\subsection{Missclassifications}
Here are some random missclassifications.

<<echo=FALSE, message=FALSE, warning=FALSE>>=
############################ -
# Find some missclassifications ----
############################ -
preds <- preds_original
random_wrongs <- filter(preds, correct==0)
set.seed(23)
random_wrongs <- random_wrongs[sample(size = 10,x=dim(random_wrongs)[1]),]

############################ -
# Plot missclassifications ----
############################ -

for (ii in 1:10){
  
  id <- paste(random_wrongs[ii,"subject_id"])
  preds <- random_wrongs[ii,"preds_all"]
  preds <- fromJSON(paste("[",gsub(pattern = "'", "\"", x=as.character(preds[[1]])),"]",sep=""))
  preds <- melt(preds,value.name = "prob",variable.name = "class")
  sub <- subjects[id]
  url_file <- unlist(sub[[id]]['urls'])[1]
  label <- unlist(sub[[id]]['label'])
  file_name <- paste(path_scratch,"image_",ii,".jpeg",sep="")
  download.file(url_file, destfile = file_name, mode = 'wb')
  img <- readJPEG(file_name)
  gg1 <- ggplot(data.frame(x=0:1,y= 0:1),aes(x=x,y=y), geom="blank") +
  annotation_custom(rasterGrob(img, width=unit(1,"npc"), height=unit(1,"npc")), 
                    -Inf, Inf, -Inf, Inf) + theme_minimal() +
  theme(axis.title = element_blank(), axis.text = element_blank()) +
  theme(plot.margin = unit(c(0.7,0.9,0,0.9), "cm"))
  
  # keep only top 5
  preds <- preds[order(preds$prob, decreasing = TRUE)[1:min(5,dim(preds)[1])],]
  gg2 <- ggplot(preds, aes(x=reorder(class, prob),y=prob)) + geom_bar(stat="identity", fill="lightblue") +
    coord_flip() +
    theme_light() +
    ylab("Predicted Probability") +
    xlab("") +
    theme(axis.text.y=element_blank(), axis.text.x=element_text(size=16),
          axis.title.x=element_text(size=16),
          axis.title.y=element_text(size=16),
          axis.ticks.y = element_blank()) +
    geom_text(aes(label=class, y=0.05), size=5,fontface="bold", vjust="middle", hjust="left") +
    theme(plot.margin = unit(c(0.7,0.9,0.5,0.9), "cm"), panel.border=element_rect(fill=NA)) +
    scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
    labs(x=NULL)
  title=textGrob(label = label,gp=gpar(fontsize=20,fontface="bold"), vjust=1)
  grid.arrange(gg1,gg2,top=title)
  grid.rect(.5,.5,width=unit(0.99,"npc"), height=unit(0.99,"npc"), 
            gp=gpar(lwd=3, fill=NA, col="black"))

}
@
\end{document}